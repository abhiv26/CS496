{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "IQ_pkyNtd4qp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhv2ehLkOKVH",
        "outputId": "5ab16dcb-1e39-4f70-9262-507b7c3f16e3"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d8810131c90>"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shakespeare karpathy dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK9ZTk6id-oC",
        "outputId": "eb23d180-95d6-4804-c3f3-b5e24b59de2e"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-21 02:43:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.12’\n",
            "\n",
            "\rinput.txt.12          0%[                    ]       0  --.-KB/s               \rinput.txt.12        100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-21 02:43:16 (25.3 MB/s) - ‘input.txt.12’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "G19o0aP1fZ6J"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num of chars:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duTebYwbffSs",
        "outputId": "0fdea925-c7d4-4e81-e088-461c8b0e8fae"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of chars: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "eJwm6Q1WfjAd",
        "outputId": "b85d8ef1-c472-4ef3-ced9-bc8f423ac7f8"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars) # our vocab size is all the unique characters used in the Shakespeare data\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrY0g1I7f_Z2",
        "outputId": "ec628a39-14eb-4d64-86c3-d95852f7c399"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to tokenize our characters by mapping each character to a number\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder takes a string, outputs a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder takes a list of integers, outputs a string"
      ],
      "metadata": {
        "id": "a28uubjcheXP"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello\"))\n",
        "print(decode(encode(\"hello\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888ub4wLjREf",
        "outputId": "e4579236-45ba-4188-e3fc-3d81649e200d"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now encode the entire text and save it to a tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data.shape, data.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqxQFeG_jkPj",
        "outputId": "5c4d2dda-e029-427b-93bb-4b000b0c873c"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1115394]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into train and val sets\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "print(len(train), len(val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBvKTgGmlPQy",
        "outputId": "429e5721-15f6-4a2f-e5d8-bc1d479d8845"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train[:block_size+1]\n",
        "# We use a sequence length of 8, but we take the data up to and including sequence length# + 1\n",
        "# because we want to know for 8 chars, what the pred for next char should be"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUuMKK9JlR-O",
        "outputId": "0b277ccc-5cca-41e9-fb7a-200ac09fec94"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train[:block_size]\n",
        "y = train[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context}, the target is: {block_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uauX05tSnaqt",
        "outputId": "61f144c1-b17b-45eb-a076-274246ed8454"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]), the target is: 8\n",
            "When input is tensor([18, 47]), the target is: 8\n",
            "When input is tensor([18, 47, 56]), the target is: 8\n",
            "When input is tensor([18, 47, 56, 57]), the target is: 8\n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is: 8\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 8\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 8\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to generate batches to process in parallel\n",
        "batch_size = 4 # how many batches do we want to process in parallel\n",
        "block_size = 8 # maximum sequence length for predictions\n",
        "\n",
        "def get_batch(data, batch_size, block_size):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  indices = torch.randint(0, len(data)-block_size, (batch_size, )) # Creating random starting indices for however many batches we want\n",
        "  x = torch.stack([data[i : i + block_size] for i in indices])\n",
        "  y = torch.stack([data[i + 1 : i + block_size + 1] for i in indices])\n",
        "  return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "sgn1Gkiwquds"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT uses a decoder-only structure from the transformer, which doesn't include cross-attention\n",
        "# First we are going to start with the input embedding\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x)\n",
        "\n",
        "# Takes tensor of shape (batch size, seq len) -> (batch size, seq len, embedding dim)"
      ],
      "metadata": {
        "id": "U6hPM6V-1chm"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len):\n",
        "    super().__init__()\n",
        "    encoding_matrix = torch.zeros(max_seq_len, d_model)\n",
        "    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1) # column vec of positions for vectorization\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "    # Each index in positional encoding accounts for TWO dimensions in an embedding vector\n",
        "    # What I do here is create the denominator term for the even set of indices,\n",
        "    # then apply it to both even and odd dimensions in the embeding\n",
        "    encoding_matrix[:, 0::2] = torch.sin(position*div_term) # even indices, start at 0 stepsize 2\n",
        "    encoding_matrix[:, 1::2] = torch.cos(position*div_term) # odd indices, start at 1 stepsize 2\n",
        "    self.register_buffer(\"pos_encoding\", encoding_matrix)\n",
        "  def forward(self, x):\n",
        "    return x + self.pos_encoding[:x.size(1)] # Return positional encodings for the target sequence length"
      ],
      "metadata": {
        "id": "TbKxj0gd7yMj"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    d_k = k.size(-1) # Q and K have shape (batch_size, n heads, seq_len, head dim)\n",
        "    scores = q@k.transpose(-2, -1) / math.sqrt(d_k) # (batch_size, n heads, q_len, k_len)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = self.dropout(F.softmax(scores, dim=-1)) # use last dim because we want to sum across keys to get probs for each query\n",
        "    return attn@v # (batch_size, n_heads, q_len, head dim)"
      ],
      "metadata": {
        "id": "EAST_IFW86Db"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, max_seq_len, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_model // n_heads\n",
        "    self.W_qkv = nn.Linear(d_model, 3*d_model) # One matrix for q, k, v for computation efficiency\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.mask = self.register_buffer(\"bias\", torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
        "                    .unsqueeze(0).unsqueeze(1))\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    qkv = self.W_qkv(x)\n",
        "\n",
        "    q, k, v = torch.split(qkv, self.d_model, dim=-1) # Split into q, k, v\n",
        "\n",
        "    batch_size_q, seq_len_q, d_model_q = q.shape\n",
        "    batch_size_k, seq_len_k, d_model_k = k.shape\n",
        "    batch_size_v, seq_len_v, d_model_v = v.shape\n",
        "\n",
        "    q = q.reshape(batch_size_q, seq_len_q, self.n_heads, self.d_head).transpose(1, 2)\n",
        "      # The original shape of Q here is (batch size, seq len, embedding dim)\n",
        "      # We want to split the embedding dim into n_heads x d_head, or dimension of each head for a number of heads\n",
        "    k = k.reshape(batch_size_k, seq_len_k, self.n_heads, self.d_head).transpose(1, 2)\n",
        "    v = v.reshape(batch_size_v, seq_len_v, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    mask = self.bias[:, :, :seq_len_q, :seq_len_q]\n",
        "\n",
        "    attn = ScaledDotProductAttention(self.dropout_rate)(q, k, v, mask) # Shape (batch size, # of heads, seq_len, # embedding dim per head)\n",
        "    attn_concat = attn.transpose(1, 2).reshape(batch_size_q, seq_len_q, d_model_q) # return axes to original posns before reshaping\n",
        "    mha_output = self.W_o(attn_concat)\n",
        "\n",
        "    return mha_output # shape (batch size, seq len, embedding dimension)"
      ],
      "metadata": {
        "id": "v2dZWfEH9bzk"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z1 = self.linear1(x)\n",
        "    a1 = self.dropout(F.gelu(z1))\n",
        "    z2 = self.linear2(a1)\n",
        "    return z2"
      ],
      "metadata": {
        "id": "DWl6Uvyb9g6T"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, d_model, eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True) # keep dim to maintain tensor dimensionality\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "# In batch normalization, each feature is normalized across all samples (like axis = 0)\n",
        "# In layer normalization, all features are normalized across each sample (like axis = 1)"
      ],
      "metadata": {
        "id": "ElS33ifv-Cer"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout_rate, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.masked_attention = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout_rate)\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.ff = FeedForward(d_model, d_ff, dropout_rate)\n",
        "    self.norm2 = LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    norm_x = self.norm1(x)\n",
        "    x = x + self.dropout(self.masked_attention(norm_x))\n",
        "    norm_x = self.norm2(x)\n",
        "    x = x + self.dropout(self.ff(norm_x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "xW8neNcY_ogk"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, n_heads, dropout_rate, vocab_size, n_layers, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.embedding = InputEmbedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "    self.decoders = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout_rate, max_seq_len) for _ in range(n_layers)])\n",
        "\n",
        "    self.final_layernorm = LayerNorm(d_model)\n",
        "\n",
        "    self.linear_output = nn.Linear(d_model, vocab_size)\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "\n",
        "    batch_size, seq_len = x.shape\n",
        "\n",
        "    token_embedding = self.embedding(x)\n",
        "    final_input = self.pos_encoding(token_embedding) # batch size, seq_len, embedding dim\n",
        "\n",
        "    for decoder in self.decoders:\n",
        "      final_input = decoder(final_input)\n",
        "\n",
        "    final_input = self.final_layernorm(final_input)\n",
        "    logits = self.linear_output(final_input)\n",
        "\n",
        "    if targets is not None:\n",
        "      batch_size, seq_len, vocabulary_size = logits.shape\n",
        "      logits = logits.reshape(batch_size*seq_len, vocabulary_size)\n",
        "      targets = targets.reshape(batch_size*seq_len)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "    else:\n",
        "      return logits\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx = idx[:, -self.max_seq_len:]\n",
        "      logits = self.forward(idx) # shape batch size, seq len, vocab size\n",
        "      logits_final_token = logits[:, -1, :] # shape of batch size, vocab size (we selected predictions for the next token position)\n",
        "      probs = F.softmax(logits_final_token, dim=-1) # now we want probabilities of the next token across the vocab size\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # shape batch size, seq len + 1\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jm9FyhqJDhX4"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MiniGPT(\n",
        "    d_model=128,\n",
        "    d_ff=512,\n",
        "    n_heads=4,\n",
        "    dropout_rate=0.1,\n",
        "    vocab_size=vocab_size,\n",
        "    n_layers=4,\n",
        "    max_seq_len=128\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1, betas=(0.9, 0.95))\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for step in range(50000):  # adjust iterations as needed\n",
        "    # Training step\n",
        "    x, y = get_batch(train, batch_size=64, block_size=128)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    model.train()\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    if step % 1000 == 0:  # Check validation every 100 steps\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_x, val_y = get_batch(val, batch_size=64, block_size=128)\n",
        "            val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "            _, val_loss = model(val_x, val_y)\n",
        "\n",
        "        print(f\"Step {step} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
        "        '''\n",
        "        # Early stopping check\n",
        "        if val_loss.item() < best_val_loss:\n",
        "            best_val_loss = val_loss.item()\n",
        "            patience_counter = 0\n",
        "            # Save the best model\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at step {step}\")\n",
        "                break\n",
        "        '''\n",
        "    elif step % 1000 == 0:\n",
        "        print(f\"Step {step} | Train Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66e79GJsiMAV",
        "outputId": "b7c10440-c549-497d-865f-8a734201fc83"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 | Train Loss: 4.3809 | Val Loss: 3.8654\n",
            "Step 1000 | Train Loss: 1.6600 | Val Loss: 1.7971\n",
            "Step 2000 | Train Loss: 1.5093 | Val Loss: 1.6260\n",
            "Step 3000 | Train Loss: 1.4156 | Val Loss: 1.5905\n",
            "Step 4000 | Train Loss: 1.4412 | Val Loss: 1.5884\n",
            "Step 5000 | Train Loss: 1.3924 | Val Loss: 1.5652\n",
            "Step 6000 | Train Loss: 1.3527 | Val Loss: 1.5075\n",
            "Step 7000 | Train Loss: 1.3056 | Val Loss: 1.5243\n",
            "Step 8000 | Train Loss: 1.3457 | Val Loss: 1.5510\n",
            "Step 9000 | Train Loss: 1.3386 | Val Loss: 1.4693\n",
            "Step 10000 | Train Loss: 1.3119 | Val Loss: 1.5515\n",
            "Step 11000 | Train Loss: 1.2770 | Val Loss: 1.5687\n",
            "Step 12000 | Train Loss: 1.2805 | Val Loss: 1.5022\n",
            "Step 13000 | Train Loss: 1.2907 | Val Loss: 1.4635\n",
            "Step 14000 | Train Loss: 1.2766 | Val Loss: 1.4772\n",
            "Step 15000 | Train Loss: 1.3011 | Val Loss: 1.4474\n",
            "Step 16000 | Train Loss: 1.2673 | Val Loss: 1.4834\n",
            "Step 17000 | Train Loss: 1.2783 | Val Loss: 1.5263\n",
            "Step 18000 | Train Loss: 1.2649 | Val Loss: 1.4947\n",
            "Step 19000 | Train Loss: 1.2581 | Val Loss: 1.5291\n",
            "Step 20000 | Train Loss: 1.2810 | Val Loss: 1.5130\n",
            "Step 21000 | Train Loss: 1.2682 | Val Loss: 1.4857\n",
            "Step 22000 | Train Loss: 1.2651 | Val Loss: 1.4536\n",
            "Step 23000 | Train Loss: 1.2350 | Val Loss: 1.4884\n",
            "Step 24000 | Train Loss: 1.2483 | Val Loss: 1.5396\n",
            "Step 25000 | Train Loss: 1.2540 | Val Loss: 1.4698\n",
            "Step 26000 | Train Loss: 1.2608 | Val Loss: 1.4794\n",
            "Step 27000 | Train Loss: 1.2567 | Val Loss: 1.4447\n",
            "Step 28000 | Train Loss: 1.2600 | Val Loss: 1.4644\n",
            "Step 29000 | Train Loss: 1.2628 | Val Loss: 1.5206\n",
            "Step 30000 | Train Loss: 1.2633 | Val Loss: 1.4554\n",
            "Step 31000 | Train Loss: 1.2222 | Val Loss: 1.4747\n",
            "Step 32000 | Train Loss: 1.2338 | Val Loss: 1.4532\n",
            "Step 33000 | Train Loss: 1.2328 | Val Loss: 1.4640\n",
            "Step 34000 | Train Loss: 1.2476 | Val Loss: 1.4676\n",
            "Step 35000 | Train Loss: 1.2230 | Val Loss: 1.4412\n",
            "Step 36000 | Train Loss: 1.2764 | Val Loss: 1.4173\n",
            "Step 37000 | Train Loss: 1.2616 | Val Loss: 1.4742\n",
            "Step 38000 | Train Loss: 1.2482 | Val Loss: 1.4762\n",
            "Step 39000 | Train Loss: 1.2700 | Val Loss: 1.4560\n",
            "Step 40000 | Train Loss: 1.2444 | Val Loss: 1.4947\n",
            "Step 41000 | Train Loss: 1.2263 | Val Loss: 1.4853\n",
            "Step 42000 | Train Loss: 1.2516 | Val Loss: 1.4734\n",
            "Step 43000 | Train Loss: 1.2470 | Val Loss: 1.5239\n",
            "Step 44000 | Train Loss: 1.2665 | Val Loss: 1.4916\n",
            "Step 45000 | Train Loss: 1.2261 | Val Loss: 1.4459\n",
            "Step 46000 | Train Loss: 1.2312 | Val Loss: 1.4920\n",
            "Step 47000 | Train Loss: 1.2198 | Val Loss: 1.4515\n",
            "Step 48000 | Train Loss: 1.2340 | Val Loss: 1.4920\n",
            "Step 49000 | Train Loss: 1.2117 | Val Loss: 1.5063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the answer to life, the universe, and everything?\"\n",
        "prompt = torch.tensor([encode(prompt)], dtype = torch.long).to(device)\n",
        "prompt.shape\n",
        "generated_idx = model.generate(prompt, max_new_tokens=100)\n",
        "output = decode(generated_idx[0].tolist())\n",
        "print(output)"
      ],
      "metadata": {
        "id": "AEgnUBKKP3_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240fc62b-b878-4f49-d9ab-fe3ef0762d53"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the universe, and everything?\n",
            "\n",
            "MONTAGUE:\n",
            "The duke of trust, I pray the suffsind\n",
            "A friend, and doking?\n",
            "Corioland peace. I pray you\n"
          ]
        }
      ]
    }
  ]
}